# Word2Vec 개념

### Embedding이란 무엇일까?
- 데이터를 낮은 차원의 vector로 표현하는 것을 임베딩이라고 한다.
    - sparse representation
        - 아이템 수와 벡터의 차원 수가 같게 표현하는 방법
        - 원핫 기법을 주로 쓴다.
        - 아이템 수가 많아지면 벡터의 차원이 많아진다는 단점이 있다.

    - dense representation
        - 아이템 수보다 벡터의 차원이 낮게 표현하는 방법
        - 아이템 별로 각 특성에 맞게 벡터의 차원을 줄인다.

### 단어 임베딩
- 단어를 벡터로 표현하는 방법
    - sparse representationg 벡터를 dense representation 벡터로 변환시켜준다.
- 벡터화 된 단어를 벡터 공간에 위치시켜서 각 단어의 거리를 바탕으로 단어간의 유사도를 측정할 수 있다.
- 임베딩을 위해서는 모델이 필요하다.
    - MF도 유저와 아이템에 대한 임베딩이라고 볼 수 있다. P,Q 행렬 각각이 임베딩된 벡터의 집합인 셈

### Word2Vec
- 뉴럴 네트워크를 기반으로 한 언어 모델
    - layer가 하나로 이루어진 모델이다.
- Word2Vec을 학습하기 위한 기법
    - CBOW
    - Skip-Gram
    - SGNS
- **CBOW(Continuous Bag of Words)**
    - 특정 단어를 예측하기 위해 양쪽에 있는 단어들로부터 예측하는 방법
    - 양쪽에서 각각 몇개의 단어를 이용할 것인지 window size를 정해야한다.
        - CBOW의 진행과정
            - 이미지 자료 첨부 후 내용 보충
        - CBOW의 파라미터 개수
            - 이미지 자료 첨부 후 내용 보충
- **Skip-Gram**
    - CBOW와 입력층과 출력층이 뒤바뀌어 구성되어 있음
        - 하나의 단어를 input으로 가진다.
    - CBOW 보다 성능이 더 좋다
- **SGNS(Skip-Gram with Negative Sampling)**
    - Item2Vec에 사용할 모델과 같은 모델
    - 기존의 skip-gram을 형태를 변형시킨 이진분류 모델
    - 이름에 그러면 왜 ***Negative Sampling***을 붙였나?
        - skip-gram은 주변에 있는 단어에 대해서만 sampling 했다면 SGNS는 주변에 있지 않은 단어까지 강제로 sampling(Negative Sampling)하여 나타내기 때문
        - 이때 Negative Sampling의 개수는 하이퍼파라미터이다.
    - SGNS의 구조
        - 이미지 자료 첨부 후 내용 보충

# Item2Vec 개념
- 아이템 기반 CF에 SGNS를 이용하여 Word2Vec처럼 Item2Vec을 적용했다.
- Word2Vec에 아이템을 적용하여 임베딩
    - 유저에 대한 아이템 리스트와 아이템들을 text와 단어로 가정하여 적용
    - 유저-아이템 관계를 활용하지 않는다.
- **Item2Vec 작동 과정**
    1. 유저나 세션별로 소비된 아이템에 대해 집합을 생성
        - 집합으로 변형되기 때문에 아이템에 대한 시퀀스 정보는 사라짐
    2. 동일한 집합의 아이템 쌍들에 대해 SGNS의 Positive Sampling을 진행
    3. 다른 집합의 데이터를 추가해서 Negative Sample도 구축
    4. Positive, Negative sampling된 데이터를 바탕으로 Item2Vec 모델 학습

# ANN
### Approximate Nearest Neighbor 개념
- Nearest Neighbor라는 알고리즘은 벡터간의 유사도를 비교하여 원하는 벡터를 찾는 모델이다.
- 이러한 알고리즘을 추천 시스템에 적용할 수 있다.
    - 유저가 구매한 상품들과 새로운 상품들간의 유사도 비교 후 추천
    - 특정 아이템과 유사한 상품군을 연관 추천할 때도 유사도 비교 후 추천
- **Brute Force KNN**
    - 전수 조사를 통해 K개의 근접한 neighbor를 탐색하는 기법
    - 벡터의 차원과 개수가 늘어날 수록 연산 속도가 선형적으로 증가하는 함
    - 그러면 정확도를 살짝만 포기하고 속도를 드라마틱하게 올릴 순 없나? => ANN
- 그래서 등장한 것이 ANN
    - 말 그대로 정확하진 않지만 대략적으로만 Nearest Neighbor를 찾아서 속도와 정확도 간의 tradeoff를 조정한 기법이다.

### ANNOY 
- 벡터를 여러개의 집합으로 나눈 뒤 트리 형태로 구성하여 탐색하는 기법
- **ANNOY 작동 과정**
    1. 임의의 두 점을 선택하고 두 점의 hyperplane으로 집함을 나눈다.
    2. 부분 집합을 계속해서 새로운 hyperplane을 생성하여 만든다. 이렇게 하면 부분 집합의 부분 집합의 부분 집합.. 형태로 트리 구조가 만들어진다.
    3. **이렇게 만들어진 트리는 각 노드가 나뉘어진 부분 집합으로 구성된다. -> 따라서 내가 원하는 벡터가 속한 집합에 대해서 탐색하기 위해서는 logn의 속도로 탐색이 가능하다**
    - 문제점
        - 집합을 나눌 때 실제적으로 근접한 점이 다른 집합에 속하는 경우
        - **이러한 경우 다른 노드들에 속한 벡터들도 탐색하는 방법으로 해결 가능**
            - 우선 순위 큐를 사용하여 해당 노드와 근접한 노드를 방문해서 유사도를 측정하게 된다.
            - 트리를 여러 개 생성해서 병렬적으로 탐색하는 방법을 이용할 수 있다.
- **하이퍼파라미터**
    - 트리의 개수
    - 탐색할 트리의 개수

### ANN의 다양한 종류
- **HNSW**
- **IVF**
- **PQ**



